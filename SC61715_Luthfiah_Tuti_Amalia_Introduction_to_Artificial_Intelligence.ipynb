{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luthfiah22/ArtificialIntelligence/blob/main/SC61715_Luthfiah_Tuti_Amalia_Introduction_to_Artificial_Intelligence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLEANING TEST DATA"
      ],
      "metadata": {
        "id": "Max2LgUGcEYv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvmRVD_DMuW5",
        "outputId": "d3dde3e4-9b70-47c2-ed20-3634d5e8d548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNj3cP8jM7kh",
        "outputId": "9ac2a590-0d8c-4884-b2b0-cd7c92e73bca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Di bawah ini telah diambil beberapa sampel artikel berita dari 3 macam jenis, yaitu olahraga, medis/kesehatan dan juga keuangan."
      ],
      "metadata": {
        "id": "147yHxlYIYMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_sport = \"Stunning goals, frantic action and moments of pure quality; Tuesday’s Champions League quarterfinal between Real Madrid and Manchester City simply had it all. After more than 90 minutes of breathless action, the first leg finished 3-3 in Madrid’s Estadio Santiago Bernabéu, perfectly setting up the return leg on April 17. And while there was no winner on the night, those watching the encounter were treated to a feast of action which started almost from the get go. “Amazing and that is exactly what the Champions League is about,” former Liverpool star and CBS pundit Jamie Carracgher said after the game. “We probably are looking at the two best teams in the competition, the history of Real Madrid and Manchester City, the champion at the moment. “That was just a privilege to watch and I can’t wait for the second leg.”. It all started less than two minutes into the game when City took the lead through a brilliant free kick from Bernardo Silva. As the midfielder stood over the set-piece, almost everyone in the stadium was expecting a cross into the box. Silva, though, had other ideas and went for goal instead, catching Los Blancos goalkeeper Andriy Lunin unaware. The Ukrainian shot-stopper tried to get back across to stop the effort and perhaps should have done better. The early goal set the tone for the game and it wasn’t long before the host was level. Exactly 10 minutes after Silva gave City the lead, Madrid had equalized after Eduardo Camavinga’s long-range effort took a wicked deflection off Rúben Dias. The Madrid faithful erupted in celebration and barely had time to sit down before their side had taken the lead just two minutes later. \"\n",
        "text_medical = \"As little as one or two minutes of vigorous exercise a day could lower your cancer risk, according to a new study. This activity can include power walking, climbing stairs, doing strenuous housework or playing with the kids, according to Dr. Emmanuel Stamatakis, lead author of the study that published Thursday in the journal JAMA Oncology. This report relied on data of more than 22,000 people in the UK Biobank, a large biomedical database and research resource that follows residents long term. Participants reported not regularly exercising in their leisure time, and they wore accelerometers to track their VILPA, or vigorous intermittent lifestyle physical activity, the study said. “Until recently we knew very little about activities done as part of daily living that reach vigorous intensity,” said Stamatakis, a professor of physical activity, lifestyle and population health at the Charles Perkins Centre and faculty of medicine and health at the University of Sydney in Australia, via email. Adults who incorporated about 4½ minutes of vigorous activity in short one- or two-minute bouts had more than 30% lower incidence rates of cancer, the study found. Understanding the health impact of vigorous activity in daily life is important because for many it may be more manageable, said CNN fitness contributor Dana Santas, a mind-body coach for professional athletes.\"\n",
        "text_finance = \"US stocks closed lower Thursday after the latest GDP report showed that US economic growth slowed to 1.6% in the first quarter of the year, a much weaker pace than expected. The Dow fell by 375 points, or 1%; the S&P 500 was down 0.5% and the Nasdaq Composite slid by 0.6%, as investors projected a longer wait for the first rate cut from the Federal Reserve. “This report was the worst of both worlds: economic growth is slowing and inflationary pressures are persisting,” wrote Chris Zaccarelli, chief investment officer at Independent Advisor Alliance, in a note Thursday morning. Economic growth appears to be floating back down to earth after notching a very strong second half of 2023. GDP grew by 4.9% and 3.4% in the third and fourth quarters of last year. At the same time, Thursday’s data showed that inflation accelerated in the first three months of the year: The annualized GDP chain price, which measures how much prices have gone up or down in the economy, helping to track inflation, jumped from 1.6% up to 3.1%. “The Fed wants to see inflation start coming down in a persistent manner, but the market wants to see economic growth and corporate profits increasing, so if neither are headed in the right direction then that’s going to be bad news for markets,” wrote Zaccarelli.\""
      ],
      "metadata": {
        "id": "R1YFKcpuM-92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [text_sport, text_medical, text_finance]"
      ],
      "metadata": {
        "id": "yf7P5NZ7Nern"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.Removing Symbols**"
      ],
      "metadata": {
        "id": "JmKOfaJkcbvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghapus simbol-simbol yang tidak diperlukan\n",
        "import re\n",
        "\n",
        "text_sport_1 = re.sub(r\"[-()\\\"#/@;:<>{}=\\_~|.,?\\[\\]]\", \"\", text_sport)\n",
        "text_medical_1 = re.sub(r\"[-()\\\"#/@;:<>{}=\\_~|.,?\\[\\]]\", \"\", text_medical)\n",
        "text_finance_1 = re.sub(r\"[-()\\\"#/@;:<>{}=\\_~|.,?\\[\\]]\", \"\", text_finance)"
      ],
      "metadata": {
        "id": "vjCon16CNjKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Removing Symbols** diperlukan karena teks yang diambil merupakan artikel berita yang pastinya banyak menggunakan simbol tanda baca dan yang lainnya."
      ],
      "metadata": {
        "id": "8C2Qxx1JIyAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.Tokenization, Lemmatization and Removing Stopwords**\n"
      ],
      "metadata": {
        "id": "5lXEXMUac90R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Mengnisialisasi lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Menampung semua teks yang telah dibersihkan di sini\n",
        "corpus_texts = []\n",
        "\n",
        "# Menginisialisasi set untuk menyimpan kata kunci BOW\n",
        "bow_keys = set()\n",
        "\n",
        "# Looping melalui setiap teks dalam `texts`\n",
        "for text in texts:\n",
        "    # Tokenisasi kata-kata dalam teks\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    # Lemmatisasi dan menghapus stop words\n",
        "    cleaned_words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "\n",
        "    # Menggabungkan kata-kata yang telah dibersihkan menjadi satu string\n",
        "    cleaned_text = ' '.join(cleaned_words)\n",
        "\n",
        "    # Menambahkan teks yang telah dibersihkan ke dalam corpus_texts\n",
        "    corpus_texts.append(cleaned_text)\n",
        "\n",
        "    # Menambahkan kata-kata yang telah dibersihkan ke dalam set bow_keys\n",
        "    bow_keys.update(cleaned_words)\n",
        "\n",
        "# Mengubah bow_keys kembali menjadi list untuk output\n",
        "bow_keys = list(bow_keys)\n",
        "\n",
        "# Print hasilnya\n",
        "print(bow_keys)  # Kata kunci BOW (Bag of Words)\n",
        "print(corpus_texts)  # Teks yang telah dibersihkan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQ1FBEzaNkmM",
        "outputId": "ea9c9b64-c1a1-4d66-a668-10fe4df86095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['jumped', 'biomedical', 'worst', 'team', 'long', 'large', 'Advisor', 'kick', 'manner', '375', 'leg.', 'celebration', 'point', 'set-piece', 'level', 'privilege', 'pace', 'third', 'annualized', 'unaware', 'At', 'simply', 'manageable', 'floating', 'increasing', 'growth', '17', 'S', '”', 'catching', 'stock', 'include', 'shot-stopper', 'tone', 'climbing', 'incidence', 'VILPA', 'Composite', 'gave', 'almost', 'went', 'Australia', 'two-minute', 'slowed', 'We', 'perfectly', 'It', 'pressure', 'University', 'Real', 'contributor', 'Thursday', 'taken', 'Amazing', 'quarterfinal', 'return', 'better', 'UK', '%', '3.4', 'game', 'feast', 'population', 'watch', 'took', 'said', 'wrote', 'want', 'Adults', 'Perkins', 'measure', 'equalized', 'setting', '1.6', 'faculty', 'wait', 'stair', 'intermittent', 'Andriy', 'neither', 'recently', 'people', 'CNN', ',', 'le', 'resident', 'wicked', 'frantic', 'showed', 'sit', 'term', 'GDP', 'journal', 'second', 'brilliant', 'professor', 'vigorous', 'profit', 'little', 'inflation', 'And', 'walking', 'deflection', 'CBS', 'set', '1', 'Stunning', 'Madrid', 'month', 'former', 'Participants', 'published', 'goalkeeper', 'accelerated', 'effort', 'day', 'latest', 'slowing', 'Until', 'persisting', 'faithful', 'impact', 'half', 'Chris', 'looking', 'rate', 'found', 'long-range', '.', 'Eduardo', 'incorporated', 'life', 'Manchester', 'though', 'investor', 'Rúben', '30', 'JAMA', 'accelerometer', 'expected', '4½', 'night', 'risk', 'helping', 'idea', 'midfielder', 'I', 'power', 'direction', 'pure', 'leg', '3.1', 'Estadio', 'everyone', 'knew', 'going', 'erupted', 'Fed', 'report', 'see', 'lead', ';', 'P', 'action', 'slid', 'That', 'email', 'health', '10', 'may', 'fourth', 'Reserve', 'projected', 'stop', 'Dias', 'right', 'database', 'many', 'wore', 'Economic', 'After', 'quarter', 'side', 'intensity', 'strenuous', '“', 'playing', 'one-', 'US', 'coming', 'Carracgher', 'reported', 'author', 'one', 'winner', 'earth', 'Independent', 'exercise', 'goal', 'get', 'Tuesday', 'persistent', 'Exactly', 'kid', 'new', 'weaker', 'Bernardo', 'three', 'start', 'bout', 'tried', 'activity', 'Sydney', 'Stamatakis', 'chain', 'Dr.', 'note', 'Nasdaq', 'Liverpool', 'box', 'April', 'closed', 'moment', 'morning', 'minute', '4.9', 'go', 'champion', 'host', 'mind-body', 'two', 'lower', 'important', 'City', 'cut', 'done', '90', 'probably', 'Lunin', 'Blancos', 'exactly', 'last', 'resource', 'Zaccarelli', 'physical', 'fitness', 'Federal', 'much', 'Silva', 'housework', 'Biobank', 'year', 'economic', 'notching', 'perhaps', 'market', 'As', 'investment', '&', 'officer', 'study', 'Oncology', 'encounter', 'lifestyle', 'cross', 'time', 'star', 'research', 'later', 'short', 'could', 'breathless', 'early', 'Santas', 'across', 'according', 'headed', 'gone', 'expecting', 'track', 'Emmanuel', '2023', 'stood', 'League', 'relied', 'competition', 'chief', 'Dow', 'corporate', 'athlete', 'grew', 'barely', 'regularly', 'medicine', 'world', 'This', 'news', 'bad', 'part', 'instead', 'Jamie', 'price', '3-3', 'Bernabéu', '’', 'Centre', 'coach', '0.6', 'via', 'professional', 'data', 'finished', 'history', 'Ukrainian', '22,000', 'Charles', 'exercising', 'inflationary', 'Los', 'back', 'Camavinga', 'treated', 'living', 'Understanding', 'Champions', 'reach', 'cancer', 'The', '500', 'best', 'longer', 'quality', 'watching', 'Santiago', 'daily', 'Alliance', 'started', ':', 'free', 'strong', 'stadium', 'leisure', 'economy', 'fell', 'appears', '0.5', 'pundit', 'follows', 'Dana', 'first']\n",
            "['Stunning goal , frantic action moment pure quality ; Tuesday ’ Champions League quarterfinal Real Madrid Manchester City simply . After 90 minute breathless action , first leg finished 3-3 Madrid ’ Estadio Santiago Bernabéu , perfectly setting return leg April 17 . And winner night , watching encounter treated feast action started almost get go . “ Amazing exactly Champions League , ” former Liverpool star CBS pundit Jamie Carracgher said game . “ We probably looking two best team competition , history Real Madrid Manchester City , champion moment . “ That privilege watch I ’ wait second leg. ” . It started le two minute game City took lead brilliant free kick Bernardo Silva . As midfielder stood set-piece , almost everyone stadium expecting cross box . Silva , though , idea went goal instead , catching Los Blancos goalkeeper Andriy Lunin unaware . The Ukrainian shot-stopper tried get back across stop effort perhaps done better . The early goal set tone game ’ long host level . Exactly 10 minute Silva gave City lead , Madrid equalized Eduardo Camavinga ’ long-range effort took wicked deflection Rúben Dias . The Madrid faithful erupted celebration barely time sit side taken lead two minute later .', 'As little one two minute vigorous exercise day could lower cancer risk , according new study . This activity include power walking , climbing stair , strenuous housework playing kid , according Dr. Emmanuel Stamatakis , lead author study published Thursday journal JAMA Oncology . This report relied data 22,000 people UK Biobank , large biomedical database research resource follows resident long term . Participants reported regularly exercising leisure time , wore accelerometer track VILPA , vigorous intermittent lifestyle physical activity , study said . “ Until recently knew little activity done part daily living reach vigorous intensity , ” said Stamatakis , professor physical activity , lifestyle population health Charles Perkins Centre faculty medicine health University Sydney Australia , via email . Adults incorporated 4½ minute vigorous activity short one- two-minute bout 30 % lower incidence rate cancer , study found . Understanding health impact vigorous activity daily life important many may manageable , said CNN fitness contributor Dana Santas , mind-body coach professional athlete .', 'US stock closed lower Thursday latest GDP report showed US economic growth slowed 1.6 % first quarter year , much weaker pace expected . The Dow fell 375 point , 1 % ; S & P 500 0.5 % Nasdaq Composite slid 0.6 % , investor projected longer wait first rate cut Federal Reserve . “ This report worst world : economic growth slowing inflationary pressure persisting , ” wrote Chris Zaccarelli , chief investment officer Independent Advisor Alliance , note Thursday morning . Economic growth appears floating back earth notching strong second half 2023 . GDP grew 4.9 % 3.4 % third fourth quarter last year . At time , Thursday ’ data showed inflation accelerated first three month year : The annualized GDP chain price , measure much price gone economy , helping track inflation , jumped 1.6 % 3.1 % . “ The Fed want see inflation start coming persistent manner , market want see economic growth corporate profit increasing , neither headed right direction ’ going bad news market , ” wrote Zaccarelli .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Dari proses di atas, telah didapatkan kata kunci yang menjadi Bag of Words. Selain itu, didapatkan pula **corpus_texts** yang merupakan list yang berisi teks-teks yang sudah memalui proses pembersihan, seperti tokenisasi, lemmatisasi dan penghapusan stopwords."
      ],
      "metadata": {
        "id": "BxBlOqjPKhqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3.Count Vectorizer**"
      ],
      "metadata": {
        "id": "ERBcSY5PdV3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Membuat objek CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "\n",
        "# Menggunakan objek CountVectorizer untuk fit_transform pada corpus\n",
        "word_counts = cv.fit_transform(corpus_texts).toarray()\n",
        "\n",
        "# Melihat hasil Count Vectorizer\n",
        "print(word_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p9nZWhVYfAc",
        "outputId": "6fa5acfd-635b-48c3-a404-f7bca29dcae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 1 0 0 0 0 0 0 1 0 0 0 1 3 0 0 0 1 0 2 1 1 1 0 0 1 1 0 0 0 0 1 0 1 1\n",
            "  1 1 1 0 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 1 2 0 0 0 4 0 0 0 0 0 1 0 0 0 0 1\n",
            "  0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 1 2 0 0 1 1 1 1 1 2 0 0 0 1 0 1 1 0 0 0\n",
            "  1 1 0 0 0 1 0 0 1 1 3 1 0 2 1 3 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 0 0\n",
            "  0 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 1 3 2 3 0 1 0 0 0 1 0 2 0 1 1 0 1\n",
            "  5 0 2 0 0 0 0 0 0 1 0 4 2 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0\n",
            "  1 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 2 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
            "  1 0 1 1 0 2 1 0 1 0 1 3 1 1 0 0 0 1 0 0 1 0 2 0 1 1 1 0 0 0 1 0 1 1 0 1\n",
            "  3 0 0 1 0 0 1 1 2 0 1 1 1 3 0 1 1 0 0 0 0 0 0 0 1 0 0 1 1 1 0 1 1 1 0 0\n",
            "  0 0 0 0]\n",
            " [1 0 0 0 1 1 0 1 0 0 0 1 2 0 0 6 1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 0 0 0 0\n",
            "  0 0 0 1 1 0 1 1 0 0 0 0 2 0 0 0 0 1 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 1 0\n",
            "  0 2 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0\n",
            "  0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 1 0 1 1 1 1 1 0 0\n",
            "  0 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 1 0 0 1 0 1 2 2 0 1 1 0 0 0 2 0\n",
            "  0 1 0 0 1 0 1 0 1 0 1 3 0 0 0 0 0 0 1 0 0 0 0 0 1 2 0 1 1 1 0 0 1 0 0 2\n",
            "  0 1 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 0\n",
            "  3 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 0 0 0 1 0 4 0 1 0 0 1 0\n",
            "  0 0 2 0 0 1 1 0 0 1 0 0 0 2 1 0 0 1 1 1 0 1 5 1 0 1 0 0 0 0 0 0 0 0 1 0\n",
            "  0 0 0 0]\n",
            " [0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 1 0 0 0 1 1 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 0 1 0 0\n",
            "  1 0 0 1 0 0 0 0 1 0 1 0 0 1 4 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 1 1\n",
            "  0 3 0 1 0 0 0 1 0 0 0 0 3 0 0 0 0 1 1 1 4 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1\n",
            "  3 1 0 0 0 1 1 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0\n",
            "  0 0 0 1 0 2 0 1 0 0 0 0 0 1 1 2 1 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0\n",
            "  0 0 1 0 0 1 2 0 0 0 0 1 1 0 0 0 0 2 0 0 1 0 0 0 0 0 2 0 0 1 0 0 0 1 0 0\n",
            "  0 0 0 1 2 0 0 0 0 2 0 0 0 0 1 1 1 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  3 1 1 0 1 3 1 0 0 1 0 0 0 0 0 0 0 0 0 0 2 0 0 0 1 0 2 0 0 0 1 0 0 0 0 1\n",
            "  1 2 3 2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Proses Count Vectorizer merupakan proses merepresentasikan teks ke dalam bentuk matriks, di mana **setiap baris mewakili satu teks dari corpus_texts** dan **setiap kolom mewakili frekuensi kemunculan kata tertentu dari kamus kata-kata yang telah dipelajari dari corpus_texts**. Setiap nilai dalam matriks ini adalah frekuensi kemunculan sebuah kata dalam teks yang bersangkutan."
      ],
      "metadata": {
        "id": "ATEU7thTLwyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEXT CLASSIFICATION"
      ],
      "metadata": {
        "id": "g4UrNt9LH2Ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Menginisialisasi lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Mendefinisikan teks yang akan dites\n",
        "query_text = \"Cristiano Ronaldo came off the bench to earn Manchester United a hard-fought 2-1 victory at Everton in the Premier League on Sunday, taking his career goal tally to 700 in the process. Just as United did last weekend in their derby mauling at the hands of local rivals Manchester City, they again found themselves behind early on at Goodison Park after Alex Iwobi curled a sublime strike into the net from 20 metres.\"\n",
        "\n",
        "# Tokenisasi kata-kata dalam teks yang akan dites\n",
        "query_words = word_tokenize(query_text)\n",
        "\n",
        "# Membersihkan kata-kata dalam teks yang akan dites\n",
        "query_words_clean = [lemmatizer.lemmatize(word) for word in query_words if word not in set(stopwords.words('english'))]\n",
        "\n",
        "# Memfilter kata-kata pada teks tersebut yang ada dalam set kata kunci BOW\n",
        "query_words_corpus = [word for word in query_words_clean if word in bow_keys]\n",
        "\n",
        "# Menggabungkan kata-kata dari teks tersebut menjadi satu string\n",
        "query_text_corpus = ' '.join(query_words_corpus)\n",
        "\n",
        "# Menambahkan teks yang akan dites ke dalam corpus_texts\n",
        "corpus_texts.append(query_text_corpus)\n",
        "\n",
        "# Menginisialisasi objek CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "\n",
        "# Transformasi teks menjadi vektor BOW\n",
        "bow_vectors = cv.fit_transform(corpus_texts).toarray()\n",
        "\n",
        "# Cetak vektor BOW\n",
        "print(bow_vectors)\n",
        "\n",
        "# Cetak panjang vektor BOW pertama (jumlah fitur)\n",
        "print(len(bow_vectors[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adkcAzWtZBUi",
        "outputId": "7e4df1fd-e762-46fd-fee7-564e6766ab47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 1 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 2 3 2]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "328\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Melakukan pembersihan pada teks yang akan dites dan menyesuaikan setiap katanya dengan kata-kata yang ada di dalam Bag of Words."
      ],
      "metadata": {
        "id": "l_YYcyAGNeCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalisasi vektor BOW\n",
        "bow_texts_norm = []\n",
        "for bow in bow_vectors:\n",
        "    length = (sum(i*i for i in bow)) ** 0.5\n",
        "    bow_norm = bow / length\n",
        "    bow_texts_norm.append(bow_norm)\n",
        "\n",
        "# Komputasi similarity menggunakan dot product\n",
        "similarity_vector = []\n",
        "bow_norm_query = bow_texts_norm[-1]  # Menggunakan vektor BOW terakhir sebagai query\n",
        "for bow in bow_texts_norm[:-1]:  # Looping sampai indeks sebelum terakhir\n",
        "    similarity_vector.append(sum(i*j for i,j in zip(bow, bow_norm_query)))\n",
        "\n",
        "# Menampilkan similarity vector\n",
        "print(similarity_vector)\n",
        "\n",
        "# Menemukan similarity tertinggi\n",
        "id_max_sim = similarity_vector.index(max(similarity_vector))\n",
        "if id_max_sim == 0:\n",
        "    print(\"The query text is classified as: Sport\")\n",
        "elif id_max_sim == 1:\n",
        "    print(\"The query text is classified as: Medical\")\n",
        "elif id_max_sim == 2:\n",
        "    print(\"The query text is classified as: Finance\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EHdw1N9Z3Hv",
        "outputId": "8a4c6be9-2ff8-42e7-9cc3-505e614a0224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.2599734734478726, 0.020161945963637795, 0.02171861213815347]\n",
            "The query text is classified as: Sport\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Proses di atas merupakan beberapa operasi untuk normalisasi vektor BOW, perhitungan similarity antara teks query dan teks dalam corpus, serta menemukan similarity tertinggi. Jadi dari proses tersebut akan mendapatkan implementasi langkah-langkah untuk membandingkan query_text dengan corpus_ext dan menentukan kategori yang paling sesuai berdasarkan similarity.\n",
        "\n",
        "* ***Hingga akhirnya didapatkan similarity (kemiripan) tertinggi dari query_text adalah dengan text_sport.***"
      ],
      "metadata": {
        "id": "HhpxCYYAP5mm"
      }
    }
  ]
}